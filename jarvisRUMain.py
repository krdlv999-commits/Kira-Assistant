import torch
import sounddevice as sd
import time
import datetime
import os
import sys
import webbrowser
import psutil 
import queue
import urllib.parse
import json
import winsound
import random
import threading
import cv2
import base64
import traceback
import pyperclip 
import screen_brightness_control as sbc 
import pyautogui 
import re 
from num2words import num2words 
from ctypes import cast, POINTER
from comtypes import CLSCTX_ALL
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume 
from openai import OpenAI
from deep_translator import GoogleTranslator
from rapidfuzz import process, fuzz
from vosk import Model, KaldiRecognizer
from duckduckgo_search import DDGS 
from dotenv import load_dotenv

# --- 1. –ù–ê–°–¢–†–û–ô–ö–ò ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    print("!!! –û–®–ò–ë–ö–ê: –ù–µ—Ç –∫–ª—é—á–∞ –≤ .env !!!")
else:
    print(f"–ö–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω: {OPENAI_API_KEY[:5]}...")

CHROME_PATH = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
MIC_ID = 1      
CAMERA_ID = 0   

THINKING_PHRASES = ["–°–º–æ—Ç—Ä—é...", "–ò–∑—É—á–∞—é...", "–°–µ–∫—É–Ω–¥—É...", "–°–µ–π—á–∞—Å –ø—Ä–æ–≤–µ—Ä—é...", "–ú–∏–Ω—É—Ç–∫—É..."]

# --- –ù–ê–°–¢–†–û–ô–ö–ê –ú–û–î–ï–õ–ï–ô (–°–∞–º–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Å–≤—è–∑–∫–∞) ---
# 1. –¢–µ–∫—Å—Ç: DeepSeek V3 (–£–º–Ω—ã–π –∏ –¥–µ—à–µ–≤—ã–π)
CHAT_MODELS = [
    "deepseek/deepseek-chat",  
    "openai/gpt-4o-mini"       
]

# 2. –ó—Ä–µ–Ω–∏–µ: GPT-4o-mini (–°–∞–º–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫)
# –ú—ã —É–±—Ä–∞–ª–∏ Gemini, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ –∏–Ω–æ–≥–¥–∞ —Å–±–æ–∏—Ç —á–µ—Ä–µ–∑ —ç—Ç–æ—Ç API
VISION_MODEL = "openai/gpt-4o-mini"

WAKE_WORDS = ["–∫–∏—Ä–∞", "–∫–∏—Ä—É", "–∫–∏—Ä–µ", "kira", "—é—Ä–∞", "–∏—Ä–∞"] 

MEMORY_FILE = "memory.json" 
USER_DATA_FILE = "user_data.json" 
CHAT_HISTORY = [] 
q = queue.Queue() 

# --- 2. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ---
if not os.path.exists("model"): sys.exit("–û–®–ò–ë–ö–ê: –ù–µ—Ç –ø–∞–ø–∫–∏ model!")
vosk_model = Model("model")

print("–ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ–ª–æ—Å–∞...")
device = torch.device('cpu')
local_file = 'model_silero.pt'
if not os.path.isfile(local_file):
    torch.hub.download_url_to_file('https://models.silero.ai/models/tts/ru/v4_ru.pt', local_file)  
model_tts = torch.package.PackageImporter(local_file).load_pickle("tts_models", "model")
model_tts.to(device)

sys.stdout.reconfigure(encoding='utf-8')
client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=OPENAI_API_KEY)

try: webbrowser.register('chrome', None, webbrowser.BackgroundBrowser(CHROME_PATH))
except: pass

volume_control = None
try:
    from comtypes import CoInitialize
    CoInitialize()
    devices = AudioUtilities.GetSpeakers()
    interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
    volume_control = cast(interface, POINTER(IAudioEndpointVolume))
except: pass

MANUAL_APPS = {
    "–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä": "calc.exe", "–±–ª–æ–∫–Ω–æ—Ç": "notepad.exe", "cmd": "cmd.exe",
    "—Ö—Ä–æ–º": CHROME_PATH, "–±—Ä–∞—É–∑–µ—Ä": CHROME_PATH, "–¥–∏—Å–ø–µ—Ç—á–µ—Ä": "taskmgr.exe",
    "—Ç–µ–ª–µ–≥—Ä–∞–º": os.path.expandvars(r"%APPDATA%\Telegram Desktop\Telegram.exe")
}
SYSTEM_APPS = {}
CORRECTIONS = {
    "led the": "led zeppelin", "—Ö–º–º": "—Ö—Ä–æ–º", "–æ—Ç–∫—Ä–æ–π —Ö–º–º": "–∑–∞–ø—É—Å—Ç–∏ —Ö—Ä–æ–º",
    "–æ–∫—Ä—É–≥–µ": "–≤ —Ä—É–∫–µ", "—á—Ç–æ —É –º–µ–Ω—è –æ–∫—Ä—É–≥–µ": "—á—Ç–æ —É –º–µ–Ω—è –≤ —Ä—É–∫–µ",
    "–ø—É—Ç—å": "–ø—É–ª—å—Ç", "—ç—Ç–æ –Ω–µ –ø—É—Ç—å": "—ç—Ç–æ –Ω–µ –ø—É–ª—å—Ç", "–≤—ã—Ö–æ–¥–∫": "–≤—ã—Ö–æ–¥",
    "–∞ —Ç–µ–ø–µ—Ä—å": "—á—Ç–æ —ç—Ç–æ", "–∞ —Å–µ–π—á–∞—Å": "—á—Ç–æ —ç—Ç–æ", "–ø–æ—Å–º–æ—Ç—Ä–∏ –µ—â–µ —Ä–∞–∑": "—á—Ç–æ —ç—Ç–æ",
    "–∞–π–¥–∞—Ä": "–∞ —ç—Ç–æ", "–∞–π –¥–∞": "–∞ —ç—Ç–æ", "–≥–æ—Ä—é": "—Å–º–æ—Ç—Ä–∏", 
    "–∫—Ä–∏–¥–∞": "–∫–∏—Ä–∞", "—á—Ç–æ –∏ —Ç–∞–º": "—á—Ç–æ —ç—Ç–æ", "—á—Ç–æ —É –º–µ–Ω—è –≤ —Ä—É–∫–∏": "—á—Ç–æ —É –º–µ–Ω—è –≤ —Ä—É–∫–µ",
    "–∏—Ä–∞ —Ç–æ –º–Ω–µ": "–∫–∏—Ä–∞ —á—Ç–æ –º–Ω–µ", "—é—Ä–∞": "–∫–∏—Ä–∞", "–ª—É—á –∫–∞–º–µ—Ä—É": "–≤–∫–ª—é—á–∏ –∫–∞–º–µ—Ä—É", "–±—é—Ä–æ": "–∫–∏—Ä–∞"
}

def load_corrections():
    if not os.path.exists(MEMORY_FILE): return CORRECTIONS
    try:
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return {**CORRECTIONS, **data}
    except: return CORRECTIONS

CORRECTIONS = load_corrections()

# --- 2.1 –ü–ê–ú–Ø–¢–¨ ---
def load_user_data():
    if not os.path.exists(USER_DATA_FILE): return {}
    try:
        with open(USER_DATA_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except: return {}

def save_user_data(key, value):
    data = load_user_data()
    data[key] = value
    with open(USER_DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

USER_FACTS = load_user_data()

# --- 3. –ó–†–ï–ù–ò–ï ---
class CameraEye:
    def __init__(self):
        self.running = False
        self.cap = None
        self.current_frame = None
        self.current_status = "–ö–∞–º–µ—Ä–∞ –≤—ã–∫–ª—é—á–µ–Ω–∞"
        self.model = None 
        self.translations = {
            "person": "—á–µ–ª–æ–≤–µ–∫", "cell phone": "—Ç–µ–ª–µ—Ñ–æ–Ω", "cup": "—á–∞—à–∫–∞", "bottle": "–±—É—Ç—ã–ª–∫–∞",
            "keyboard": "–∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞", "mouse": "–º—ã—à–∫–∞", "remote": "–ø—É–ª—å—Ç", "book": "–∫–Ω–∏–≥–∞",
            "laptop": "–Ω–æ—É—Ç–±—É–∫", "scissors": "–Ω–æ–∂–Ω–∏—Ü—ã", "pen": "—Ä—É—á–∫–∞", "spoon": "–ª–æ–∂–∫–∞",
            "baseball bat": "–±–∏—Ç–∞/–ø–∞–ª–∫–∞", "toothbrush": "–∑—É–±–Ω–∞—è —â–µ—Ç–∫–∞"
        }

    def _load_model(self):
        if self.model is None:
            print(">>> –ü–æ–¥–≥—Ä—É–∂–∞—é YOLO...")
            from ultralytics import YOLO 
            self.model = YOLO("yolov8s.pt")

    def start(self):
        if self.running: return
        self._load_model()
        self.running = True
        self.thread = threading.Thread(target=self._update_loop)
        self.thread.daemon = True
        self.thread.start()

    def stop(self):
        self.running = False
        self.current_status = "–ö–∞–º–µ—Ä–∞ –≤—ã–∫–ª—é—á–µ–Ω–∞"
        if self.cap: self.cap.release()
        cv2.destroyAllWindows()

    def get_snapshot_base64(self):
        if not self.running:
            self._load_model()
            cap = cv2.VideoCapture(CAMERA_ID)
            for _ in range(5): ret, frame = cap.read()
            cap.release()
            if not ret: return None
        elif self.current_frame is None:
            return None
        else:
            frame = self.current_frame.copy()
        _, buffer = cv2.imencode('.jpg', frame, [int(cv2.IMWRITE_JPEG_QUALITY), 50])
        return base64.b64encode(buffer).decode('utf-8')

    def _update_loop(self):
        self.cap = cv2.VideoCapture(CAMERA_ID)
        while self.running:
            ret, frame = self.cap.read()
            if not ret: break
            self.current_frame = frame 
            results = self.model(frame, verbose=False, stream=True, conf=0.45)
            detected_objects = []
            for r in results:
                frame = r.plot()
                for box in r.boxes:
                    obj_name = self.model.names[int(box.cls[0])]
                    detected_objects.append(self.translations.get(obj_name, obj_name))
            if detected_objects:
                counts = {i:detected_objects.count(i) for i in detected_objects}
                self.current_status = ", ".join([f"{k}" if v==1 else f"{k} ({v})" for k,v in counts.items()])
            else: self.current_status = "–ü—É—Å—Ç–æ"
            cv2.imshow("Kira Vision (YOLO)", frame)
            if cv2.waitKey(1) == ord('q'): self.stop(); break

kira_eye = CameraEye()

def audio_callback(indata, frames, time, status):
    if status: print(status, file=sys.stderr)
    q.put(bytes(indata))

def play_sound(type):
    try:
        if type == "wake": winsound.Beep(1000, 200)
        elif type == "end": winsound.Beep(1500, 100)
        elif type == "sleep": winsound.Beep(700, 300)
    except: pass

def speak(text):
    if not text: return
    print(f"\nKira: {text}") 
    try:
        def replace_numbers(match):
            return num2words(int(match.group()), lang='ru')
        clean_text = re.sub(r'\d+', replace_numbers, text)
        clean_text = clean_text.replace("<s>", "").replace("*", "")
        clean_text = clean_text.replace("#", "").replace("**", "") 
        
        audio = model_tts.apply_tts(text=clean_text, speaker='xenia', sample_rate=48000, put_accent=True, put_yo=True)
        sd.play(torch.cat((audio, torch.zeros(int(48000 * 0.5)))), 48000)
        sd.wait()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ TTS: {e}")

def scan_installed_apps():
    global SYSTEM_APPS
    paths = [os.path.join(os.environ['PROGRAMDATA'], r'Microsoft\Windows\Start Menu\Programs'), os.path.join(os.environ['APPDATA'], r'Microsoft\Windows\Start Menu\Programs')]
    for path in paths:
        if not os.path.exists(path): continue
        for root, dirs, files in os.walk(path):
            for file in files:
                if file.endswith(".lnk"):
                    SYSTEM_APPS[file.lower().replace(".lnk", "").replace("  ", " ")] = os.path.join(root, file)

def open_in_chrome(url):
    try: webbrowser.get('chrome').open(url)
    except: webbrowser.open(url)

def search_internet(query):
    print(f"üåç –ò—â—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {query}")
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=3))
            if results:
                return "\n".join([f"- {r['title']}: {r['body']}" for r in results])
    except: pass
    return None

# --- –ú–û–ó–ì ---
def ask_gpt_text(prompt):
    global CHAT_HISTORY, USER_FACTS
    visual_context = kira_eye.current_status
    user_context = f"–§–∞–∫—Ç—ã –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ: {json.dumps(USER_FACTS, ensure_ascii=False)}" if USER_FACTS else ""
    
    now = datetime.datetime.now()
    date_str = now.strftime("%d.%m.%Y")
    time_str = now.strftime("%H:%M")
    
    print(f"–î—É–º–∞—é... (–í–∏–∂—É: {visual_context})", end='\r')
    
    system_prompt = (
        f"–¢—ã –ö–∏—Ä–∞. –°–ï–ì–û–î–ù–Ø: {date_str}, –í–†–ï–ú–Ø: {time_str}. "
        f"–¢–≤–æ–∏ –≥–ª–∞–∑–∞ –≤–∏–¥—è—Ç: {visual_context}. {user_context}. "
        f"–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –ó–ê–ü–û–ú–ù–ò–¢–¨, –≤–µ—Ä–Ω–∏: MEMORY: <–∫–ª—é—á>|<–∑–Ω–∞—á–µ–Ω–∏–µ>. "
        f"–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏/–ø–æ–≥–æ–¥—É (–∏ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å), –≤–µ—Ä–Ω–∏: SEARCH: <–∑–∞–ø—Ä–æ—Å>. "
        f"–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º."
    )

    for model in CHAT_MODELS:
        try:
            messages = [{"role": "system", "content": system_prompt}] + CHAT_HISTORY + [{"role": "user", "content": prompt}]
            
            completion = client.chat.completions.create(model=model, messages=messages, max_tokens=300)
            answer = completion.choices[0].message.content
            
            if "SEARCH:" in answer:
                search_query = answer.replace("SEARCH:", "").strip()
                search_results = search_internet(search_query)
                if search_results:
                    follow_up = f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: {search_results}\n–°–µ–≥–æ–¥–Ω—è {date_str}. –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ."
                    messages.append({"role": "assistant", "content": answer}) 
                    messages.append({"role": "user", "content": follow_up})
                    completion = client.chat.completions.create(model=model, messages=messages, max_tokens=300)
                    answer = completion.choices[0].message.content
                else: answer = "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ."

            elif "MEMORY:" in answer:
                try:
                    raw = answer.replace("MEMORY:", "").strip()
                    key, value = raw.split("|", 1)
                    save_user_data(key.strip(), value.strip())
                    USER_FACTS = load_user_data()
                    answer = f"–ó–∞–ø–æ–º–Ω–∏–ª–∞: {key} - {value}"
                except: answer = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç—å."

            try: answer = GoogleTranslator(source='auto', target='ru').translate(answer)
            except: pass
            
            CHAT_HISTORY.append({"role": "user", "content": prompt})
            CHAT_HISTORY.append({"role": "assistant", "content": answer})
            if len(CHAT_HISTORY) > 10: CHAT_HISTORY = CHAT_HISTORY[-10:]
            return answer
        except Exception as e:
            print(f"\n–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ {model}: {e}")
            continue
            
    return "–ú–æ–∑–≥ –æ—Ñ—Ñ–ª–∞–π–Ω (–≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)."

def ask_gpt_vision(prompt):
    global CHAT_HISTORY
    print("–î–µ–ª–∞—é —Å–Ω–∏–º–æ–∫...", end='\r')
    base64_image = kira_eye.get_snapshot_base64()
    if not base64_image: return "–ù–µ –≤–∏–∂—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."
    try:
        messages = [{
            "role": "user",
            "content": [
                {"type": "text", "text": f"–ü–æ—Å–º–æ—Ç—Ä–∏ –∏ –æ—Ç–≤–µ—Ç—å: {prompt}. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø–æ-—Ä—É—Å—Å–∫–∏."},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
            ]
        }]
        # –î–õ–Ø –ó–†–ï–ù–ò–Ø –ò–°–ü–û–õ–¨–ó–£–ï–ú GPT-4o-mini (–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å 100%)
        completion = client.chat.completions.create(model=VISION_MODEL, messages=messages, max_tokens=300)
        answer = completion.choices[0].message.content
        CHAT_HISTORY.append({"role": "user", "content": f"[–§–æ—Ç–æ: {prompt}]"})
        CHAT_HISTORY.append({"role": "assistant", "content": f"[–ù–∞ —Ñ–æ—Ç–æ: {answer}]"})
        return answer
    except Exception as e:
        print(f"\n–û–®–ò–ë–ö–ê –ó–†–ï–ù–ò–Ø: {e}") 
        return "–ó—Ä–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ."

# --- –ö–û–ú–ê–ù–î–´ ---
def execute_command(query):
    if not query: return
    print(f"\n–ö–æ–º–∞–Ω–¥–∞: {query}")
    play_sound("end")
    for w, r in CORRECTIONS.items():
        if w in query: query = query.replace(w, r)
    if any(x in query for x in ['–æ—Ç–±–æ–π', '—Ö–≤–∞—Ç–∏—Ç', '—Å—Ç–æ–ø', '–∑–∞–º–æ–ª—á–∏']):
         speak("–û–∫.")
         return "SLEEP_NOW" 
    
    if '–±—É—Ñ–µ—Ä' in query or '—Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω' in query:
        if '–∏—Å–ø—Ä–∞–≤—å' in query or '–ø—Ä–æ–≤–µ—Ä—å' in query or '–æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π' in query:
            text_to_fix = pyperclip.paste()
            if not text_to_fix: speak("–ë—É—Ñ–µ—Ä –ø—É—Å—Ç."); return
            speak("–ò—Å–ø—Ä–∞–≤–ª—è—é...")
            prompt = f"–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏ –∏ —É–ª—É—á—à–∏: {text_to_fix}"
            fixed_text = ask_gpt_text(prompt)
            pyperclip.copy(fixed_text)
            speak("–ì–æ—Ç–æ–≤–æ.")
            return

    if '–ø–∞—É–∑–∞' in query or '–ø—Ä–æ–¥–æ–ª–∂' in query: pyautogui.press('playpause'); return
    if '—Å–ª–µ–¥—É—é—â–∏–π —Ç—Ä–µ–∫' in query: pyautogui.press('nexttrack'); return
    if '–ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ç—Ä–µ–∫' in query: pyautogui.press('prevtrack'); return
    if '—Å–≤–µ—Ä–Ω–∏ –≤—Å–µ' in query: pyautogui.hotkey('win', 'd'); return

    vision_triggers = ["—á—Ç–æ —ç—Ç–æ", "—á—Ç–æ —è –¥–µ—Ä–∂—É", "—á—Ç–æ –≤ —Ä—É–∫–µ", "–æ–ø–∏—à–∏", "–ø–æ—Å–º–æ—Ç—Ä–∏", "—á—Ç–æ —Ç—ã –≤–∏–¥–∏—à—å", "–∞ —ç—Ç–æ", "—Å–∫–∞–∂–∏ —á—Ç–æ —ç—Ç–æ", "–∫–∞–∫–æ–≥–æ —Ü–≤–µ—Ç–∞"]
    is_vision = False
    for t in vision_triggers:
        if fuzz.partial_ratio(t, query) > 85: is_vision = True; break     
    if is_vision:
        if not kira_eye.running: 
            kira_eye.start()
            speak("–í–∫–ª—é—á–∞—é –≥–ª–∞–∑–∞...")
            time.sleep(2) 
        speak(random.choice(THINKING_PHRASES))
        description = ask_gpt_vision(query)
        speak(description)
        return

    if '—è—Ä–∫–æ—Å—Ç—å' in query:
        try:
            current = sbc.get_brightness()[0]
            if any(x in query for x in ['–ø—Ä–∏–±–∞–≤—å', '—É–≤–µ–ª–∏—á—å', '–¥–æ–±–∞–≤—å', '–±–æ–ª—å—à–µ']): sbc.set_brightness(min(100, current + 20)); speak("–Ø—Ä—á–µ.")
            elif any(x in query for x in ['—É–±–∞–≤—å', '—É–º–µ–Ω—å—à–∏', '—Ç–∏—à–µ', '–º–µ–Ω—å—à–µ']): sbc.set_brightness(max(0, current - 20)); speak("–¢–µ–º–Ω–µ–µ.")
        except: speak("–ù–µ –º–æ–≥—É.")
        return
    if '–≥—Ä–æ–º–∫–æ—Å—Ç—å' in query or '–∑–≤—É–∫' in query:
        if volume_control:
            if any(x in query for x in ['–≥—Ä–æ–º—á–µ', '–¥–æ–±–∞–≤—å']):
                c = volume_control.GetMasterVolumeLevelScalar()
                volume_control.SetMasterVolumeLevelScalar(min(1.0, c + 0.1), None)
                speak("–ì—Ä–æ–º—á–µ.")
            elif any(x in query for x in ['—Ç–∏—à–µ', '—É–±–∞–≤—å']):
                c = volume_control.GetMasterVolumeLevelScalar()
                volume_control.SetMasterVolumeLevelScalar(max(0.0, c - 0.1), None)
                speak("–¢–∏—à–µ.")
            elif '–≤—ã–∫–ª—é—á–∏' in query: volume_control.SetMute(1, None); speak("–í—ã–∫–ª—é—á–∏–ª–∞.")
            elif '–≤–∫–ª—é—á–∏' in query: volume_control.SetMute(0, None); speak("–í–∫–ª—é—á–∏–ª–∞.")
        return
    if '–∑–∞—Ä—è–¥' in query:
        b = psutil.sensors_battery()
        if b: speak(f"–ó–∞—Ä—è–¥ {b.percent}%.")
        else: speak("–ù–µ—Ç –±–∞—Ç–∞—Ä–µ–∏.")
        return

    if '–≤–∫–ª—é—á–∏ –∫–∞–º–µ—Ä—É' in query: kira_eye.start(); speak("–ì–ª–∞–∑–∞ –æ—Ç–∫—Ä—ã—Ç—ã."); return
    if '–≤—ã–∫–ª—é—á–∏ –∫–∞–º–µ—Ä—É' in query: kira_eye.stop(); speak("–ì–ª–∞–∑–∞ –∑–∞–∫—Ä—ã—Ç—ã."); return
    if '–∫—Ç–æ —Ç—ã' in query: speak("–Ø –ö–∏—Ä–∞."); return
    if fuzz.partial_ratio("—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏", query) > 75: speak(f"–°–µ–π—á–∞—Å {datetime.datetime.now().strftime('%H:%M')}"); return
    if '—é—Ç—É' in query:
        search_term = query.replace('–≤–∫–ª—é—á–∏', '').replace('–Ω–∞–π–¥–∏', '').replace('—é—Ç—É–±', '').replace('—é—Ç—É', '').strip()
        encoded_term = urllib.parse.quote(search_term)
        open_in_chrome(f"https://www.youtube.com/results?search_query={encoded_term}")
        return
    if '–∑–∞–ø—É—Å—Ç–∏' in query or '–æ—Ç–∫—Ä–æ–π' in query:
        raw_app_name = query.replace('–∑–∞–ø—É—Å—Ç–∏', '').replace('–æ—Ç–∫—Ä–æ–π', '').replace('–ø—Ä–æ–≥—Ä–∞–º–º—É', '').strip()
        if raw_app_name:
            speak(f"–ò—â—É {raw_app_name}...") 
            ALL_APPS = {**MANUAL_APPS, **SYSTEM_APPS}
            result = process.extractOne(raw_app_name, ALL_APPS.keys(), scorer=fuzz.ratio, score_cutoff=60)
            if result:
                speak(f"–ó–∞–ø—É—Å–∫–∞—é {result[0]}")
                try: os.startfile(ALL_APPS[result[0]])
                except: speak("–û—à–∏–±–∫–∞ —Ñ–∞–π–ª–∞.")
            else: speak(f"–ù–µ –Ω–∞—à–ª–∞ {raw_app_name}")
        return
    if '–≤—ã—Ö–æ–¥' in query: kira_eye.stop(); speak("–ü–æ–∫–∞."); sys.exit()
    speak(ask_gpt_text(query))

def main():
    scan_installed_apps()
    rec = KaldiRecognizer(vosk_model, 16000)
    print(f"–ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É ID {MIC_ID}...")
    try:
        input_stream = sd.RawInputStream(samplerate=16000, blocksize=8000, device=MIC_ID, 
                                         dtype='int16', channels=1, callback=audio_callback)
    except Exception as e:
        print(f"–û–®–ò–ë–ö–ê –ú–ò–ö–†–û–§–û–ù–ê: {e}"); sys.exit()

    speak("–Ø –Ω–∞ —Å–≤—è–∑–∏. Jarvis 47.0")
    dialogue_mode = True 
    last_interaction_time = time.time()
    
    print(">>> –°–õ–£–®–ê–Æ (30 —Å–µ–∫) <<<")
    
    with input_stream:
        while True:
            try:
                data = q.get() 
                status_text = kira_eye.current_status[:40] if kira_eye.running else "OFF"
                print(f"YOLO: {status_text}... | –°–ª—É—à–∞—é... ", end='\r')
                
                if rec.AcceptWaveform(data):
                    result = json.loads(rec.Result())
                    text = result.get('text', '')

                    if text:
                        print(" " * 80, end='\r') 
                        if not dialogue_mode:
                            if any(name in text for name in WAKE_WORDS):
                                print("\n>>> –ü–†–û–°–ù–£–õ–ê–°–¨ <<<")
                                play_sound("wake")
                                dialogue_mode = True
                                last_interaction_time = time.time()
                                for name in WAKE_WORDS:
                                    if name in text:
                                        command = text.replace(name, "").strip()
                                        if len(command) > 2:
                                            res = execute_command(command)
                                            if res == "SLEEP_NOW": dialogue_mode = False
                                            else: last_interaction_time = time.time()
                                        break
                        else:
                            print(f"–°–ª—ã—à—É: {text}")
                            res = execute_command(text)
                            if res == "SLEEP_NOW": 
                                print(">>> –°–û–ù <<<")
                                play_sound("sleep")
                                dialogue_mode = False
                            else: 
                                last_interaction_time = time.time()
                
                if dialogue_mode and (time.time() - last_interaction_time > 30):
                    print("\n>>> –¢–ê–ô–ú–ê–£–¢: –£–°–ù–£–õ–ê <<<")
                    play_sound("sleep")
                    dialogue_mode = False
            except KeyboardInterrupt: break
            except Exception as e: print(f"\n–û—à–∏–±–∫–∞: {e}"); traceback.print_exc()

if __name__ == "__main__":
    main()